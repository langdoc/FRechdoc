---
params:
   author: 'Niko Partanen'
   language: 'Some language'
   project: 'Some project'
   directory: '.'
   tier_prefix: 'word@'
   eaf_filter: '.+'
author: '`r params$author`'
date: "`r Sys.Date()`"
output: html_document
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Used libraries
library(tidyverse)
library(xml2)
library(stringr)
```

## `r params$language` status report

```{r, echo=FALSE}

# This downloads the punctuation token file

punct <- read_lines('https://raw.githubusercontent.com/langdoc/FRechdoc/master/transcription/punctuation.txt')

# In principle the whole XPATH could be a parameter,
# now it is only the tier prefix up to @ character

token_xpath <- paste0("//TIER[starts-with(@TIER_ID, '", params$tier_prefix, "')]/ANNOTATION/*/ANNOTATION_VALUE")

# This finds all ELAN files

eaf_files <- dir(path = params$directory, pattern = 'eaf$', full.names = T, recursive = T)

# This builds a data frame out of them

corpus <- eaf_files %>% 
        keep(., ~ str_detect(.x, params$eaf_filter)) %>%
        map(read_xml) %>%
        discard(., ~ .x %>% 
                        xml_find_all(token_xpath) %>% 
                        length == 0) %>%
        map_df(., ~ tibble(token = .x %>% 
                                xml_find_all(token_xpath) %>% 
                                xml_text)) %>%
        bind_rows()

```

This is a report on `r params$language` corpus built within `r params$project` project.

The corpus contains currently, `r Sys.Date()`, `r nrow(corpus)` tokens, of which `r corpus %>% filter(! token %in% punct)Â %>% nrow` are word tokens.

The most common word tokens are:

```{r, echo=FALSE}

# This builds a table with most common word tokens,
# please notice quite a bit is done in this point too

corpus %>% 
        filter(! token %in% punct) %>% # remove punctuation characters
        mutate(token = str_trim(token)) %>% # trimming whitespace
        mutate(token = tolower(token)) %>% # lowercase
        filter(token != ' +') %>% # remove spaces if around
        filter(token != '') %>% # remove empty tokens
        count(token) %>% # counting tokens
        arrange(desc(n)) %>% # most common tokens first
        slice(1:10) %>% # slice ten most common
        knitr::kable()

```

